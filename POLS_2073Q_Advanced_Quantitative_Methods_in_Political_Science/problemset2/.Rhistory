# When there is strong multicollinearity between two or more predictor variables, it becomes difficult
# for the model to estimate the contribution of each individual variable to the response variable.
# As a result, the coefficient estimates for the affected variables can become unstable, and their
# standard errors can become large. This means that the precision of the coefficient estimates can
# decrease, and it can become more difficult to draw meaningful conclusions from the model. However,
# when the level of multicollinearity is not severe, the impact on the coefficient estimates and
# their precision may be small.
pop <- lm(population ~ cell_subscription + total_miles_driven, data = mydata2)
tot <- lm(total_miles_driven ~ cell_subscription + population, data = mydata2)
pop
tot
#---------------------------------------------------------------------------------------------------
# 2.5 (3.a from p 222)
# The coefficient for the cell ban dummy variable is -100.8, and it is not statistically significant
# (p-value = 0.176). This suggests that there is no evidence of a significant difference in the
# number of deaths in states with a cell phone ban compared to those without a cell phone ban, after
# controlling for other variables in the model. The coefficient for the texting ban dummy variable
# is -165.2, and it is statistically significant (p-value = 0.00578). This suggests that, on average,
# states with a texting ban have a lower number of deaths compared to those without a texting ban,
# after controlling for other variables in the model. However, it's important to note that this is
# an observational study, and therefore, it's not possible to establish causality.
model7 <- lm(numberofdeaths ~ cell_subscription + population + total_miles_driven + cell_ban + text_ban, data = mydata2)
summary(model)
#---------------------------------------------------------------------------------------------------
# 2.6 (3.b. from p 222)
# In the model from part (a), the coefficient estimate for the cell_ban variable is negative and
# statistically significant, which suggests that having a cell phone ban is associated with a
# reduction in the number of deaths due to distracted driving. However, the model does not include
# an interaction term between cell_ban and population, so it does not account for the possibility
# that the effect of a cell phone ban on the number of deaths may vary depending on the size of the
# population. Without an interaction term, the coefficient estimate for cell_ban represents the
# average effect of a cell phone ban across all states, regardless of population size. Therefore,
# the results from part (a) do not allow for the possibility that a cell phone ban saves more lives
# in a state with a large population compared to a state with a small population. To properly
# investigate the possibility of interaction, a new model can be specified that includes an
# interaction term between cell_ban and population. The coefficient estimate for the interaction
# term would represent the difference in the effect of a cell phone ban on the number of deaths
# between a state with a large population and a state with a small population. If the interaction
# term is statistically significant, it would indicate that the effect of a cell phone ban on the
# number of deaths due to distracted driving varies depending on the size of the population.
#---------------------------------------------------------------------------------------------------
# 2.7 (3.c from p 222)
# To answer these questions, we need to look at the coefficients of the interaction terms in the
# model:
# The estimated effect of a cell phone ban for California is the sum of the coefficient for cell_ban
# and the product of the coefficient for total_miles_driven and cell_ban:
# 33.64 - 0.002938 * 326271.651 = 231.59
# The estimated effect of a cell phone ban for Wyoming is simply the coefficient for cell_ban: 33.64
# The estimated effect of a texting ban for California is the sum of the coefficient for text_ban and
# the product of the coefficient for total_miles_driven and text_ban:
# -7.07 - 0.002247 * 326271.651 = -731.33
# The estimated effect of a texting ban for Wyoming is simply the coefficient for text_ban: -7.07
# The effect of total_miles_driven is the sum of the coefficient for total_miles_driven and the
# product of the coefficients for the interaction terms:
# 0.01282 + 5.694e-08 * 326271.651 - 2.938e-03 * 9270.994 - 2.247e-03 * 9270.994 = 0.00239
# These estimates suggest that a cell phone ban has a much larger effect on reducing the number of
# deaths in California compared to Wyoming. However, the effect of a texting ban is much larger in
# Wyoming than in California. Additionally, the effect of total miles driven is positive, indicating
# that more miles driven is associated with more deaths.
model8 <- lm(numberofdeaths ~ cell_subscription * total_miles_driven + cell_ban * total_miles_driven + text_ban * total_miles_driven, data = mydata2)
summary(model8)
#---------------------------------------------------------------------------------------------------
# 2.8 (3.d from pp222-223)
# Based on the results from part (c), the estimated effect of the cell phone ban for California is
# 33.64 - 0.002938 x 326271.651 = 245.8 and the estimated effect for Wyoming is
# 33.64 - 0.002938 x 9270.994 = 8.15. These values represent the y-intercept of the cell phone ban
# fitted line for California and Wyoming, respectively.
# The conditions under which the cell phone ban has a statistically significant effect are when the
# confidence intervals of the cell phone ban fitted line do not include zero. This means that there
# is strong evidence that the effect of the cell phone ban is different from zero, and therefore,
# the ban has a statistically significant effect on reducing the number of deaths. In Figure 6.14,
# the confidence intervals are depicted by the dashed lines, and if they do not overlap with zero
# for a given range of total miles, then the ban has a statistically significant effect for that
# range.
#---------------------------------------------------------------------------------------------------
# QUESTION 3
mydata3 <- read_dta("social_welfare_race(1).dta")
#---------------------------------------------------------------------------------------------------
# 3.1 The national poverty rate for a family of three in 2020 was $21,720. Based on the monthly
# welfare maximum in each state, how many states provide welfare benefit that exceeds the national
# poverty rate?
# 3.2 How many provide a benefit that is below than half of the poverty rate?
# compute annual welfare benefit
mydata3$welfare_annual <- mydata3$welfare3 * 12
# compute national poverty rate for family of three
poverty_rate <- 21720
# count number of states with welfare benefit above national poverty rate
n_above_poverty <- sum(mydata3$welfare_annual > poverty_rate)
# count number of states with welfare benefit below half the poverty rate
n_below_half_poverty <- sum(mydata3$welfare_annual < poverty_rate/2)
# print results
cat("Number of states with welfare benefit above national poverty rate:", n_above_poverty, "\n")
cat("Number of states with welfare benefit below half the poverty rate:", n_below_half_poverty, "\n")
#---------------------------------------------------------------------------------------------------
# 3.3 Many international comparisons of poverty set an poverty level to be 50% of the average wage,
# rather than having one poverty level for the entire country.  Generate a new variable whose value
# (for each state) is 50% of the average wage; call it pov_level50. (Don't forget that wages in the
# dataset are WEEKLY). What is the pov_level50 for Connecticut? $34,476
# create new variable pov_level50 and print for all states
print(mydata3$pov_level50 <- 0.5 * 52 * mydata3$avg_wkly_wage)
# print pov_level50 for Connecticut
print(subset(mydata3, state == "Connecticut")$pov_level50)
#---------------------------------------------------------------------------------------------------
# 3.4  Explain why pov_level50  might be a better poverty measure for states than using $21,720
# for all states?
# Using a fixed national poverty measure of $21,720 for all states may not be an accurate reflection
# of the actual poverty level in each state, since the cost of living and the average wage levels
# can vary significantly between different regions of the country. Using pov_level50, which is 50%
# of the average wage in each state, takes into account the differences in the cost of living and
# wage levels across different states, and thus provides a more customized and accurate measure of
# poverty for each state. This can help policymakers better understand and address the specific
# poverty issues and needs of each state.
#---------------------------------------------------------------------------------------------------
# 3.5 Based on the pov_level50 measure, how many states provide welfare benefits that are above the
# state's relative poverty rate? (Don't forget that welfare benefit is measured monthly)  5
sum(mydata3$welfare3 > 0.5 * 52 * mydata3$avg_wkly_wage * 0.5 / 12)
#---------------------------------------------------------------------------------------------------
# 3.6 How many states provide benefits that are less than half of pov_level50
# (i.e., benefit / pov_level50 <= .50) ? 45
mydata3$avg_annual_wage <- mydata3$avg_wkly_wage * 52
mydata3$pov_level50 <- 0.5 * mydata3$avg_annual_wage / 12
sum(mydata3$welfare3 / mydata3$pov_level50 <= 0.5, na.rm = TRUE)
#---------------------------------------------------------------------------------------------------
# 3.7  How far below the pov_level50 in Connecticut ; that is, the benefit level is what percentage
# of the state's poverty rate (benefit/pov_rate50 would be the ratio) 0.6982249
# filter the data for Connecticut
ct_data <- mydata3[mydata3$state == "Connecticut",]
# get the value of pov_level50 for Connecticut
ct_pov50 <- ct_data$avg_wkly_wage * 0.5 * 52 * 0.5
# get the value of the welfare benefit for Connecticut
ct_welfare <- ct_data$welfare3 * 12
# calculate the ratio of ct_welfare to ct_pov50
ct_ratio <- ct_welfare / ct_pov50
ct_ratio
#---------------------------------------------------------------------------------------------------
# 3.8 What happens if you try to estimate the model using the dummy variable for all four regions?
# If you try to estimate the model using the dummy variable for all four regions, it would result
# in perfect multicollinearity, which is a situation where one predictor variable can be perfectly
# predicted from the others. This occurs because the four dummy variables for the four regions are
# not linearly independent, since they must sum to 1 for each observation. As a result, the
# regression output would not be valid, and the estimated coefficients and standard errors would
# be unreliable. To avoid this, one of the dummy variables should be excluded as a reference
# category.
#---------------------------------------------------------------------------------------------------
# 3.9  Based on the model estimate, what is the estimated welfare benefit for a state with average
# weekly wage of $900 and a minority share of 15% in the South. 974.5683
model9 <- lm(welfare3 ~ avg_wkly_wage + minority_share + factor(region), data = mydata3)
summary(model9)
# create data frame with predictor values
newdata <- data.frame(avg_wkly_wage = 900, minority_share = 15, region = 2)
# use predict function to get predicted value from model
pred_value <- predict(model9, newdata = newdata)
pred_value
#---------------------------------------------------------------------------------------------------
# 3.10 Are any of the estimates in the model statistically significant? If so which ones. (Note,
# do not worry about the constant)
# Yes, some of the estimates in the model are statistically significant. We can look at the
# "Pr(>|t|)" column in the coefficient table to see which variables have p-values less than 0.05,
# indicating that they are statistically significant at the 5% level.
# From the output provided in question 3.9, we can see that the intercept, avg_wkly_wage,
# minority_share, and factor(region)4 have p-values less than 0.05, indicating that they are
# statistically significant at the 5% level. The factor(region)2 and factor(region)3 have p-values
# greater than 0.05, indicating that they are not statistically significant at the 5% level.
#---------------------------------------------------------------------------------------------------
# 3.11 The coefficient for "west"  in this model regress welfare3 avg_wkly_wage minority_share
# west midwest northeast, robust tells us that:
# a. western states pay 30 dollars more than states in the other regions all else equal
# b. western states pay on average 138 more than southern states all else equal
# b. western states pay on average 139 more than southern states all else equal
# estimate the model with robust standard errors
model11 <- lm(welfare3 ~ avg_wkly_wage + minority_share + factor(region), data = mydata3)
robust_model <- coeftest(model11, vcov = vcovHC(model, type = "HC1"))
# view the results
summary(model11)
summary(robust_model)
#---------------------------------------------------------------------------------------------------
# 3.12  None of the individual region estimates are statistically significant at the .05 level, but
# some are close. Calculate an F-test to determine if the dummys are jointly significant (i.e., that
# we should include the set of region variables)
# Based on the F-statistic of 1.81 and a corresponding p-value of 0.159, we fail to reject the null
# hypothesis that all region variables have zero coefficients, meaning that the set of region
# variables is not jointly significant at the 0.05 level. Therefore, we may consider dropping the
# region variables from the model.
# Model 1: without region variables
model12 <- lm(welfare3 ~ avg_wkly_wage + minority_share, data = mydata3)
# Model 2: with region variables
model13 <- lm(welfare3 ~ avg_wkly_wage + minority_share + factor(region), data = mydata3)
# calculate RSS for Model 1
RSS1 <- sum(model12$residuals^2)
# calculate RSS for Model 2
RSS2 <- sum(model13$residuals^2)
# calculate degrees of freedom
df1 <- df.residual(model12)
df2 <- df.residual(model13)
df_diff <- abs(df2 - df1)
# calculate F-statistic
F_stat <- ((RSS1 - RSS2) / df_diff) / (RSS2 / df2)
# calculate p-value
p_value <- pf(F_stat, df_diff, df2, lower.tail = FALSE)
# print results
cat("F-statistic:", F_stat, "\n")
cat("p-value:", p_value, "\n")
#---------------------------------------------------------------------------------------------------
# 3.13 Add a new column to your dataset that adds the average Republication share of the state house
# since 2010.  Estimate the previous model adding this new variable. Discuss the results. Does the
# additional of this variable substantially change your coefficient estimates? Does it change your
# inferences about whether higher minority group share reduces welfare benefit levels?
# Based on the output, the addition of the "average" variable to the model does not substantially
# change the coefficient estimates for the other variables. The coefficients for "avg_wkly_wage"
# and "minority_share" are similar in both models, and the coefficients for the "region" variables
# are only slightly different. The coefficient for "average" is negative and not statistically
# significant, suggesting that having a higher percentage of Republicans in the state general
# assembly does not have a significant effect on welfare benefit levels. Overall, the adjusted
# R-squared value for the model with the "average" variable is slightly higher than the model
# without it, indicating that the additional variable explains a small amount of the variation
# in the response variable. However, the p-value for the "average" variable suggests that it is
# not a significant predictor of welfare benefit levels, and therefore, its inclusion in the model
# may not be necessary.
# This dataset is without these states information: Missouri, Montana, Nebraska, Nevada,
# New hampshire, New Jersey, New Mexico, New York, Utah, Vermont, Virginia, Washington,
# West Virginia, Wisconsin, Wyoming
mydata4 <- read.csv("State House Partisanship 2010-22 - Sheet1.csv")
# merge the datasets
merged_data <- merge(mydata3, mydata4[, c("state", "average")], by = "state")
# create new model
model13_new <- lm(welfare3 ~ avg_wkly_wage + minority_share + factor(region) + average, data = merged_data)
summary(model13_new)
# compare
summary(model13)
summary(model13_new)
#---------------------------------------------------------------------------------------------------
# Load the required packages
library(stats)
library(sandwich)
library(lmtest)
library(haven)
library(ggplot2)
library(outliers)
library(dplyr)
# read the .dta file
mydata <- read_dta("big_height.dta")
#---------------------------------------------------------------------------------------------------
# QUESTION 1
# estimate the model
model <- lm(student ~ mother + male + ideology, data = mydata)
# get robust standard errors
se_robust <- sqrt(diag(vcovHC(model, type="HC1")))
# print the regression table with robust standard errors
coeftest(model, vcov = vcovHC(model, type="HC1"), df = Inf, sqrt(diag(vcovHC(model, type="HC1"))))
# generate the scatterplot with jitter
ggplot(mydata, aes(x = mother, y = student)) +
geom_jitter(width = 0.2, height = 0.2) +
labs(x = "Mother Height", y = "Student Height")
# count the number of observations where the student height is 80 inches or greater
n_above_80 <- sum(mydata$mother >= 80)
# print the number of observations
n_above_80
#---------------------------------------------------------------------------------------------------
# 1.1: Discuss whether there are their outliers?
# There are two clear outliers in the dataset. There are two mothers with heights over 80, which
# is very unlikely and probably due to a mistake in the input.
# create a new dataset that excludes the outliers
mydata_no_outliers <- subset(mydata, mother <= 80)
# Estimate the model with robust standard errors
model_no_outliers <- lm(student ~ mother + male + ideology, data = mydata_no_outliers)
summary(model_no_outliers)
#---------------------------------------------------------------------------------------------------
# 1.2: What is the value of an estimate of the height of a male with mother height 67 inches and
# v liberal (1)? 73.03742
# define the values for mother, male, and ideology
new_data <- data.frame(mother = 67, male = 1, ideology = 1)
# use the predict function to obtain the predicted student height
predicted_height <- predict(model, newdata = new_data)
# print the predicted height
predicted_height
#---------------------------------------------------------------------------------------------------
# 1.3: Can you give the y-hat for the height of a woman with mother's height 65 inches? Why not?
# No, we cannot give the y-hat (predicted value) for the height of a woman with mother's height 65
# inches using the model that was estimated earlier, because the model only includes variables for
# male, mother, and ideology, and does not include a variable for sex or female. Since we do not
# have any information about the sex of the student, we cannot use the model to make predictions for
# the height of a woman with mother's height 65 inches. In other words, the model is only applicable
# to male students, and cannot be used to make predictions for female students.
#---------------------------------------------------------------------------------------------------
# 1.4 What are the *residuals* for each of the four males in the dataset who have mothers with
# height 65 and who score 7 on the ideology scale (round answer to 1 decimal) 0.8, 5.3, 1.3, 1.7
# Please describe the code you used or the process by which you determined these residual values.
# Below has the commented code on how I accomplished this.
# subset the data to include only males with mother height 65 and ideology score 7
sub_data <- subset(mydata, male == 1 & mother == 65 & ideology)
# calculate the residuals using the predict function and the model object
residuals <- sub_data$student - predict(model, newdata = sub_data)
# round the residuals to one decimal place
round(residuals, 1)
#---------------------------------------------------------------------------------------------------
#***************************************************************************************************
#***************************************************************************************************
#***************************************************************************************************
# 1.5 You may recall that in the original heights dataset, we found a mysterious positive and
# significant relationship between ideology and student height... conservativism was associated with
# being taller, even after controlling for gender  What do we find in this current, larger sample?
# Which result (from the original sample or the larger sample) do you believe more and why? (You may
# want to estimate the model from the original heights dataset (available on HuskyCT) to compare to
# the estimate I the new dataset.)
# It appears that the estimated coefficient for ideology is
# negative, which suggests that higher ideology scores are associated with lower student height.
# However, the coefficient is not statistically significant (p = 0.27), which means we cannot reject
# the null hypothesis that there is no relationship between ideology and student height in the
# population. Therefore, based on this analysis, there is no evidence of a significant relationship
# between ideology and student height in the new dataset. Additionally, this is much more believable
# than the original dataset because there should not be a correlations between ideology and height.
# fit linear model
model_3 <- lm(student ~ mother + male + ideology, data = mydata)
# examine coefficients
summary(model_3)$coefficients
#***************************************************************************************************
#***************************************************************************************************
#***************************************************************************************************
#---------------------------------------------------------------------------------------------------
# QUESTION 2
mydata2 <- read_dta("Ch5_Exercise3_Cell_phone_subscriptions.dta")
#---------------------------------------------------------------------------------------------------
# 2.1 (3 a on p. 179) show the code to estimate the model here and discuss results in remark.
# The estimated bivariate regression model is:
# Number_of_deaths = 124 + 0.091*Cell_subscription
# The coefficient of cell_subscription is positive and statistically significant at the 1% level,
# indicating that an increase in the number of cell phone subscriptions is associated with an
# increase in the number of traffic deaths. The R-squared value of 0.8233 suggests that about 82.3%
# of the variation in the number of traffic deaths can be explained by the number of cell phone
# subscriptions. The residuals indicate that the model does not fit perfectly, as some of the
# residuals are quite large. This suggests that there may be other factors that are also related to
# traffic deaths, and that are not included in the model. The issue of endogeneity is not directly
# addressed by this analysis. Endogeneity occurs when the independent variable (in this case, cell
# phone subscriptions) is correlated with the error term in the model. This can lead to biased
# estimates of the coefficient. Endogeneity can be caused by omitted variable bias or simultaneity,
# among other factors. In this case, there may be other factors that are related to both cell
# phone subscriptions and traffic deaths, such as population density or the number of miles driven,
# which are not included in the model. This could lead to endogeneity, and bias the estimates of
# the coefficient.
model4 <- lm(numberofdeaths ~ cell_subscription, data = mydata2)
summary(model4)
#---------------------------------------------------------------------------------------------------
# 2.2 (3b on p 179) show the code and discuss results in a remark
# Adding population to the model improves its fit, as shown by the increase in the adjusted
# R-squared from 0.8196 to 0.8501. The coefficient on cell phone subscriptions changes from 0.09115
# to -0.2109, indicating that the relationship between cell phone subscriptions and traffic deaths
# becomes negative and significant once population is included in the model. This may be because
# population is positively related to both cell phone subscriptions and traffic deaths, so failing
# to include it in the model leads to omitted variable bias.
model5 <- lm(numberofdeaths ~ cell_subscription + population, data = mydata2)
summary(model5)
#---------------------------------------------------------------------------------------------------
# 2.3 (3 c on p 180)
# The coefficient on cell phone subscriptions became much smaller and is no longer statistically
# significant after adding total miles driven to the model. The estimate changed from -0.211 to
# 0.002, with a p-value of 0.975, indicating that there is no evidence that the number of cell phone
# subscriptions is related to traffic fatalities once we account for the effects of population and
# total miles driven.
model6 <- lm(numberofdeaths ~ cell_subscription + population + total_miles_driven, data = mydata2)
summary(model6)
#---------------------------------------------------------------------------------------------------
# 2.4 (3.d on p. 180)
# For the model in part (c), the VIF for population is 1.23, and the VIF for total miles driven is
# 2.02. These values indicate that there is some multicollinearity between the predictor variables,
# but it is not severe enough to cause major concerns. The two models in part (d) are different
# because they have different response and predictor variables. In the first model, population is
# the response variable and cell phone subscriptions and total miles driven are the predictor
# variables. In the second model, total miles driven is the response variable and cell phone
# subscriptions and population are the predictor variables. Because the roles of the predictor and
# response variables are reversed, it is possible that the relationships between the predictor
# variables and the response variable are also different. The presence of multicollinearity in a
# regression model can have several implications for the coefficient estimates and their precision.
# When there is strong multicollinearity between two or more predictor variables, it becomes difficult
# for the model to estimate the contribution of each individual variable to the response variable.
# As a result, the coefficient estimates for the affected variables can become unstable, and their
# standard errors can become large. This means that the precision of the coefficient estimates can
# decrease, and it can become more difficult to draw meaningful conclusions from the model. However,
# when the level of multicollinearity is not severe, the impact on the coefficient estimates and
# their precision may be small.
pop <- lm(population ~ cell_subscription + total_miles_driven, data = mydata2)
tot <- lm(total_miles_driven ~ cell_subscription + population, data = mydata2)
pop
tot
#---------------------------------------------------------------------------------------------------
# 2.5 (3.a from p 222)
# The coefficient for the cell ban dummy variable is -100.8, and it is not statistically significant
# (p-value = 0.176). This suggests that there is no evidence of a significant difference in the
# number of deaths in states with a cell phone ban compared to those without a cell phone ban, after
# controlling for other variables in the model. The coefficient for the texting ban dummy variable
# is -165.2, and it is statistically significant (p-value = 0.00578). This suggests that, on average,
# states with a texting ban have a lower number of deaths compared to those without a texting ban,
# after controlling for other variables in the model. However, it's important to note that this is
# an observational study, and therefore, it's not possible to establish causality.
model7 <- lm(numberofdeaths ~ cell_subscription + population + total_miles_driven + cell_ban + text_ban, data = mydata2)
summary(model)
model7 <- lm(numberofdeaths ~ cell_subscription + population + total_miles_driven + cell_ban + text_ban, data = mydata2)
summary(model)
View(mydata2)
View(mydata2)
model7 <- lm(numberofdeaths ~ cell_subscription + population + total_miles_driven + cell_ban + text_ban, data = mydata2)
summary(model7)
model8 <- lm(numberofdeaths ~ cell_subscription * total_miles_driven + cell_ban * total_miles_driven + text_ban * total_miles_driven, data = mydata2)
summary(model8)
model8 <- lm(numberofdeaths ~ cell_ban * total_miles_driven + text_ban * total_miles_driven, data = mydata2)
summary(model8)
mydata3 <- read_dta("social_welfare_race(1).dta")
#---------------------------------------------------------------------------------------------------
# 3.1 The national poverty rate for a family of three in 2020 was $21,720. Based on the monthly
# welfare maximum in each state, how many states provide welfare benefit that exceeds the national
# poverty rate?
# 3.2 How many provide a benefit that is below than half of the poverty rate?
# compute annual welfare benefit
mydata3$welfare_annual <- mydata3$welfare3 * 12
# compute national poverty rate for family of three
poverty_rate <- 21720
# count number of states with welfare benefit above national poverty rate
n_above_poverty <- sum(mydata3$welfare_annual > poverty_rate)
# count number of states with welfare benefit below half the poverty rate
n_below_half_poverty <- sum(mydata3$welfare_annual < poverty_rate/2)
# print results
cat("Number of states with welfare benefit above national poverty rate:", n_above_poverty, "\n")
cat("Number of states with welfare benefit below half the poverty rate:", n_below_half_poverty, "\n")
#---------------------------------------------------------------------------------------------------
# 3.3 Many international comparisons of poverty set an poverty level to be 50% of the average wage,
# rather than having one poverty level for the entire country.  Generate a new variable whose value
# (for each state) is 50% of the average wage; call it pov_level50. (Don't forget that wages in the
# dataset are WEEKLY). What is the pov_level50 for Connecticut? $34,476
# create new variable pov_level50 and print for all states
print(mydata3$pov_level50 <- 0.5 * 52 * mydata3$avg_wkly_wage)
# print pov_level50 for Connecticut
print(subset(mydata3, state == "Connecticut")$pov_level50)
#---------------------------------------------------------------------------------------------------
# 3.4  Explain why pov_level50  might be a better poverty measure for states than using $21,720
# for all states?
# Using a fixed national poverty measure of $21,720 for all states may not be an accurate reflection
# of the actual poverty level in each state, since the cost of living and the average wage levels
# can vary significantly between different regions of the country. Using pov_level50, which is 50%
# of the average wage in each state, takes into account the differences in the cost of living and
# wage levels across different states, and thus provides a more customized and accurate measure of
# poverty for each state. This can help policymakers better understand and address the specific
# poverty issues and needs of each state.
#---------------------------------------------------------------------------------------------------
# 3.5 Based on the pov_level50 measure, how many states provide welfare benefits that are above the
# state's relative poverty rate? (Don't forget that welfare benefit is measured monthly)  5
sum(mydata3$welfare3 > 0.5 * 52 * mydata3$avg_wkly_wage * 0.5 / 12)
#---------------------------------------------------------------------------------------------------
# QUESTION 3
mydata3 <- read_dta("social_welfare_race(1).dta")
#---------------------------------------------------------------------------------------------------
# 3.1 The national poverty rate for a family of three in 2020 was $21,720. Based on the monthly
# welfare maximum in each state, how many states provide welfare benefit that exceeds the national
# poverty rate?
# 3.2 How many provide a benefit that is below than half of the poverty rate?
# compute annual welfare benefit
mydata3$welfare_annual <- mydata3$welfare3 * 12
# compute national poverty rate for family of three
poverty_rate <- 21720
# count number of states with welfare benefit above national poverty rate
n_above_poverty <- sum(mydata3$welfare_annual > poverty_rate)
# count number of states with welfare benefit below half the poverty rate
n_below_half_poverty <- sum(mydata3$welfare_annual < poverty_rate/2)
# print results
cat("Number of states with welfare benefit above national poverty rate:", n_above_poverty, "\n")
cat("Number of states with welfare benefit below half the poverty rate:", n_below_half_poverty, "\n")
#---------------------------------------------------------------------------------------------------
# 3.3 Many international comparisons of poverty set an poverty level to be 50% of the average wage,
# rather than having one poverty level for the entire country.  Generate a new variable whose value
# (for each state) is 50% of the average wage; call it pov_level50. (Don't forget that wages in the
# dataset are WEEKLY). What is the pov_level50 for Connecticut? $34,476
# create new variable pov_level50 and print for all states
print(mydata3$pov_level50 <- 0.5 * 52 * mydata3$avg_wkly_wage)
# print pov_level50 for Connecticut
print(subset(mydata3, state == "Connecticut")$pov_level50)
#---------------------------------------------------------------------------------------------------
# 3.4  Explain why pov_level50  might be a better poverty measure for states than using $21,720
# for all states?
# Using a fixed national poverty measure of $21,720 for all states may not be an accurate reflection
# of the actual poverty level in each state, since the cost of living and the average wage levels
# can vary significantly between different regions of the country. Using pov_level50, which is 50%
# of the average wage in each state, takes into account the differences in the cost of living and
# wage levels across different states, and thus provides a more customized and accurate measure of
# poverty for each state. This can help policymakers better understand and address the specific
# poverty issues and needs of each state.
#---------------------------------------------------------------------------------------------------
#***************************************************************************************************
#***************************************************************************************************
#***************************************************************************************************
# 3.5 Based on the pov_level50 measure, how many states provide welfare benefits that are above the
# state's relative poverty rate? (Don't forget that welfare benefit is measured monthly)  5
sum(mydata3$welfare3 > 0.5 * 52 * mydata3$avg_wkly_wage * 0.5 / 12)
#***************************************************************************************************
#***************************************************************************************************
#***************************************************************************************************
#---------------------------------------------------------------------------------------------------
# 3.6 How many states provide benefits that are less than half of pov_level50
# (i.e., benefit / pov_level50 <= .50) ? 45
mydata3$avg_annual_wage <- mydata3$avg_wkly_wage * 52
mydata3$pov_level50 <- 0.5 * mydata3$avg_annual_wage / 12
sum(mydata3$welfare3 / mydata3$pov_level50 <= 0.5, na.rm = TRUE)
#---------------------------------------------------------------------------------------------------
# 3.7  How far below the pov_level50 in Connecticut ; that is, the benefit level is what percentage
# of the state's poverty rate (benefit/pov_rate50 would be the ratio) 0.6982249
# filter the data for Connecticut
ct_data <- mydata3[mydata3$state == "Connecticut",]
# get the value of pov_level50 for Connecticut
ct_pov50 <- ct_data$avg_wkly_wage * 0.5 * 52 * 0.5
# get the value of the welfare benefit for Connecticut
ct_welfare <- ct_data$welfare3 * 12
# calculate the ratio of ct_welfare to ct_pov50
ct_ratio <- ct_welfare / ct_pov50
ct_ratio
View(merged_data)

<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.313">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>softmax_neural_tensorflow</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="softmax_neural_tensorflow_files/libs/clipboard/clipboard.min.js"></script>
<script src="softmax_neural_tensorflow_files/libs/quarto-html/quarto.js"></script>
<script src="softmax_neural_tensorflow_files/libs/quarto-html/popper.min.js"></script>
<script src="softmax_neural_tensorflow_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="softmax_neural_tensorflow_files/libs/quarto-html/anchor.min.js"></script>
<link href="softmax_neural_tensorflow_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="softmax_neural_tensorflow_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="softmax_neural_tensorflow_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="softmax_neural_tensorflow_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="softmax_neural_tensorflow_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">



<section id="multiclass-classification-with-softmax-regression-and-neural-networks-with-tensorflow" class="level1">
<h1>Multiclass Classification with Softmax Regression and Neural Networks with <code>Tensorflow</code></h1>
<section id="what-is-a-multiclass-classification-problem" class="level2">
<h2 class="anchored" data-anchor-id="what-is-a-multiclass-classification-problem">1. What is a multiclass classification problem?</h2>
<p>A multiclass classification problem is a type of supervised learning problem in machine learning, where the goal is to predict the class or category of an input observation, based on a set of known classes. In a multiclass classification problem, there are more than two possible classes, and the algorithm must determine which of the possible classes the input observation belongs to.</p>
<p>For example, if we want to classify images of animals into different categories, such as dogs, cats, and horses, we have a multiclass classification problem. In this case, the algorithm must learn to distinguish between the different features of each animal to correctly identify its class.</p>
<p>Multiclass classification problems can be solved using various algorithms, such as logistic regression, decision trees, support vector machines, and neural networks. The performance of these algorithms is typically evaluated using metrics such as accuracy, precision, recall, and F1 score.</p>
</section>
<section id="what-is-softmax-regression" class="level2">
<h2 class="anchored" data-anchor-id="what-is-softmax-regression">2. What is softmax regression?</h2>
<p>Softmax regression is a type of logistic regression that is often used for multiclass classification problems. In a multiclass classification problem, the goal is to predict the class of an input observation from a set of possible classes. Softmax regression provides a way to model the probabilities of the input observation belonging to each of the possible classes.</p>
<p>In softmax regression, the model’s output is a vector of probabilities that represent the likelihood of the input observation belonging to each of the possible classes. The softmax function is used to map the output of the linear regression model to a probability distribution over the classes, ensuring that the probabilities of all classes sum to one.</p>
<p>The goal of softmax regression is to predict the probability of an input observation belonging to each of the possible classes. To achieve this, we compute the weighted sum of the input features <span class="math inline">\(\vec{x}\)</span> with a weight vector <span class="math inline">\(\vec{w}_j\)</span> for each class <span class="math inline">\(j\)</span>, and add a bias term <span class="math inline">\(b_j\)</span>. This gives us a scalar value <span class="math inline">\(z_j\)</span> for each class <span class="math inline">\(j\)</span>. We then apply the softmax function to the <span class="math inline">\(z\)</span> values to obtain a probability distribution over the possible classes.</p>
<p>More specifically, for a given input observation, we compute the scalar values <span class="math inline">\(z_j\)</span> for all <span class="math inline">\(N\)</span> classes as follows:</p>
<p><span class="math display">\[
z_j = \vec{w}_j \cdot \vec{x} + b_j,\ \forall j \in {1, 2, \ldots, N}
\]</span></p>
<p>Or, in our example: <span class="math display">\[
z_1 = \vec{w}_1 \cdot \vec{x} + b_1
\]</span> <span class="math display">\[
z_2 = \vec{w}_2 \cdot \vec{x} + b_2
\]</span> <span class="math display">\[
z_3 = \vec{w}_3 \cdot \vec{x} + b_3
\]</span></p>
<p>The equations provided describe how the model computes a set of scores for each class, which are used to compute the probability distribution over the classes.</p>
<p>Each of the equations describes a linear regression model that computes a score, <span class="math inline">\(z_i\)</span>, for class <span class="math inline">\(i\)</span> based on the input vector <span class="math inline">\(\vec{x}\)</span> and a set of weights, <span class="math inline">\(\vec{w}_i\)</span>, and bias, <span class="math inline">\(b_i\)</span>.</p>
<p>Scores refer to the scalar values <span class="math inline">\(z_j\)</span> computed for each class <span class="math inline">\(j\)</span>. These scores represent the model’s confidence in the input observation belonging to each of the possible classes, and are used to compute the final probability distribution over the classes.</p>
<p>For example, if we have three classes (1, 2, and 3), and the model computes scores of 0.7, 0.2, and 0.1 for each class respectively, this would indicate that the model is most confident that the input observation belongs to class 1, but has lower confidence that it belongs to classes 2 and 3. The probabilities computed from these scores would reflect this relative confidence as well.</p>
<p>Let’s break down the first equation in the example:</p>
<p><span class="math display">\[z_1 = \vec{w}_1 \cdot \vec{x} + b_1\]</span></p>
<p>Here, <span class="math inline">\(\vec{w}_1\)</span> is a vector of weights that corresponds to the input features. Each weight represents the importance of a feature in determining the score for class 1. The dot product of the weight vector and input vector, <span class="math inline">\(\vec{w}_1 \cdot \vec{x}\)</span>, is a weighted sum of the input features that determines the contribution of each feature to the score. The bias term, <span class="math inline">\(b_1\)</span>, represents a constant offset that can be used to shift the scores for class 1 up or down.</p>
<p>The scores for classes 2 and 3 are computed using similar equations, but with different weight vectors and biases. The scores can be positive or negative, depending on the input features and the weight values. The sign and magnitude of the scores determine which classes are more likely to be predicted by the model.</p>
<p>To convert the scores to a probability distribution over the classes, the model uses the softmax function, which takes the exponent of each score and normalizes them to sum up to 1. The softmax function outputs a vector of probabilities, where each element represents the probability of the input belonging to a specific class. <span class="math display">\[
a_j = \frac{e^{z_j}}{\sum\limits_{k=1}^N e^{z_k}} = P(y=j \mid \vec{x})
\]</span></p>
<p>Or again, in our example: <span class="math display">\[
a_1 = \frac{e^{z_1}}{e^{z_1} + e^{z_2} + e^{z_3}} = P(y = 1| \vec{x})
\]</span> <span class="math display">\[
a_2 = \frac{e^{z_2}}{e^{z_1} + e^{z_2} + e^{z_3}} = P(y = 2| \vec{x})
\]</span> <span class="math display">\[
a_3 = \frac{e^{z_3}}{e^{z_1} + e^{z_2} + e^{z_3}} = P(y = 3| \vec{x})
\]</span></p>
<section id="estimation" class="level4">
<h4 class="anchored" data-anchor-id="estimation">Estimation</h4>
<p>In softmax regression, the cost function is used to measure how well the model predicts the probability of an input belonging to each of the possible classes. The goal is to find the set of weights and biases that minimize the cost function, which measures the difference between the predicted probabilities and the true labels.</p>
<p>The cross-entropy loss is a commonly used cost function for softmax regression. The cross-entropy loss is a measure of the dissimilarity between the predicted probability distribution and the true probability distribution. The cross-entropy loss is given by the following two identical formulas:</p>
<p><span class="math display">\[
J(\vec{w},b) = -\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{k}y_{ij}\log(\hat{y}_{ij})
\]</span> where <span class="math inline">\(\hat{y}_{ij}\)</span> is the predicted probability of example <span class="math inline">\(i\)</span> belonging to class <span class="math inline">\(j\)</span> <span class="math display">\[
\text{loss}(a_1, a_2, \dots, a_n, y) =
\begin{cases}
-\log(a_1) &amp; \text{if } y = 1 \\
-\log(a_2) &amp; \text{if } y = 2 \\
\vdots &amp; \vdots \\
-\log(a_n) &amp; \text{if } y = n
\end{cases}
\]</span></p>
<p>Using our example: <span class="math display">\[
\text{loss}(a_1, a_2, a_3, y) = \begin{cases}
-\log(a_1) &amp; \text{if } y = 1 \\
-\log(a_2) &amp; \text{if } y = 2 \\
-\log(a_3) &amp; \text{if } y = 3
\end{cases}
\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the number of training examples, <span class="math inline">\(k\)</span> is the number of possible classes, <span class="math inline">\(y_{ij}\)</span> is the true label for example <span class="math inline">\(i\)</span> and class <span class="math inline">\(j\)</span>, and <span class="math inline">\(\hat{y}_{ij}\)</span> is the predicted probability for example <span class="math inline">\(i\)</span> and class <span class="math inline">\(j\)</span>.</p>
<p>The cross-entropy loss can be interpreted as the average number of bits needed to represent the true distribution of the classes given the predicted distribution. A lower cross-entropy loss indicates that the predicted probabilities are closer to the true probabilities.</p>
<p>During training, the model adjusts the weights and biases to minimize the cross-entropy loss. This is typically done using an optimization algorithm such as gradient descent. The gradient of the cost function with respect to the weights and biases is computed, and the weights and biases are updated in the direction of the negative gradient to reduce the cost.</p>
<p>In summary, the cost function in softmax regression measures the difference between the predicted probability distribution and the true label distribution, and is used to train the model to make better predictions by adjusting the weights and biases to minimize the cost.</p>
<p>Here is a visualization of the loss function in python:</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the range of values for the predicted probability</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>y_hat <span class="op">=</span> np.arange(<span class="fl">0.001</span>, <span class="fl">1.0</span>, <span class="fl">0.01</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the true label as 1 for this example</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute the cross-entropy loss for each value of y_hat</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>loss <span class="op">=</span> <span class="op">-</span> y_true <span class="op">*</span> np.log(y_hat) <span class="op">-</span> (<span class="dv">1</span> <span class="op">-</span> y_true) <span class="op">*</span> np.log(<span class="dv">1</span> <span class="op">-</span> y_hat)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the loss function</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>plt.plot(y_hat, loss)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Predicted probability'</span>)</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Cross-entropy loss'</span>)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="softmax_neural_tensorflow_files/figure-html/cell-2-output-1.png" width="576" height="422"></p>
</div>
</div>
<p>In this example, we define a range of values for the predicted probability, y_hat, and set the true label to 1 for simplicity. We then compute the cross-entropy loss for each value of y_hat using the formula for the cross-entropy loss. Finally, we plot the loss function as a function of the predicted probability.</p>
<p>The resulting plot should show a U-shaped curve, with the minimum value of the loss occurring at a predicted probability of 1.0 for the true class and 0.0 for the other class.</p>
<p>The U-shaped curve of the cross-entropy loss function is a reflection of the way the loss function penalizes incorrect predictions. The intuition behind this shape is as follows:</p>
<p>If the model correctly predicts the probability of the true class to be 1.0 (i.e., the predicted probability distribution perfectly matches the true label distribution), then the loss function evaluates to 0.0. This is the minimum possible value of the loss function, and corresponds to the best possible prediction.</p>
<p>As the predicted probability of the true class decreases from 1.0, the loss function begins to increase. This reflects the increasing penalty for incorrectly predicting the probability of the true class.</p>
<p>As the predicted probability of the true class approaches 0.0, the loss function increases very rapidly. This reflects the fact that the model is very confident in an incorrect prediction, and the penalty for this kind of error is very high.</p>
<p>Similarly, as the predicted probability of the true class approaches 1.0 from below, the loss function increases very rapidly again. This reflects the fact that the model is not confident enough in the correct prediction, and the penalty for this kind of error is also very high.</p>
<p>Finally, as the predicted probability of the true class approaches 1.0 from above, the loss function begins to increase more slowly again. This reflects the fact that the model is becoming more confident in the correct prediction, and the penalty for being slightly off is lower than for being very wrong.</p>
<p>Overall, the U-shaped curve of the cross-entropy loss function reflects the way that the model is penalized for incorrect predictions. The loss function is high when the model is very confident in an incorrect prediction, or not confident enough in a correct prediction, and is low when the model makes a perfect prediction.</p>
<p>We can also visualize what we expect a softmax regression model to look like:</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate random data</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.random.randn(<span class="dv">300</span>, <span class="dv">2</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.random.choice([<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>], <span class="dv">300</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Add bias term</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.hstack([np.ones((X.shape[<span class="dv">0</span>], <span class="dv">1</span>)), X])</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Define softmax function</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> softmax(X):</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    expX <span class="op">=</span> np.exp(X)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> expX <span class="op">/</span> np.<span class="bu">sum</span>(expX, axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize weights</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>W <span class="op">=</span> np.random.randn(X.shape[<span class="dv">1</span>], <span class="dv">3</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Train softmax regression</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>num_iterations <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_iterations):</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>    P <span class="op">=</span> softmax(X.dot(W))</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    gradient <span class="op">=</span> X.T.dot(P <span class="op">-</span> np.eye(<span class="dv">3</span>)[y])</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>    W <span class="op">-=</span> learning_rate <span class="op">*</span> gradient</span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute predicted classes</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> np.argmax(X.dot(W), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot data and decision boundaries</span></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>plt.scatter(X[:, <span class="dv">1</span>], X[:, <span class="dv">2</span>], c<span class="op">=</span>y, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>x_min, x_max <span class="op">=</span> plt.xlim()</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>y_min, y_max <span class="op">=</span> plt.ylim()</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>xx, yy <span class="op">=</span> np.meshgrid(np.linspace(x_min, x_max, <span class="dv">100</span>), np.linspace(y_min, y_max, <span class="dv">100</span>))</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> np.argmax(np.hstack([np.ones((xx.size, <span class="dv">1</span>)), xx.ravel()[:, np.newaxis], yy.ravel()[:, np.newaxis]]).dot(W), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> Z.reshape(xx.shape)</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>plt.contourf(xx, yy, Z, alpha<span class="op">=</span><span class="fl">0.3</span>, levels<span class="op">=</span><span class="dv">3</span>, cmap<span class="op">=</span><span class="st">'viridis'</span>)</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="softmax_neural_tensorflow_files/figure-html/cell-3-output-1.png" width="569" height="404"></p>
</div>
</div>
<p>This code generates a scatter plot of the data points, with each class represented by a different color. It then plots the decision boundaries of the classifier, which are represented by the colored regions. The regions are created by classifying a large grid of points that spans the plot, and then plotting the regions of the grid that correspond to each class.</p>
</section>
</section>
<section id="neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="neural-networks">4. Neural Networks</h2>
<section id="introduction" class="level3">
<h3 class="anchored" data-anchor-id="introduction">Introduction</h3>
<p>A neural network is a type of machine learning algorithm that is inspired by the structure and function of the human brain. It consists of layers of interconnected nodes, or neurons, that can learn to recognize patterns in data and make predictions or decisions based on that input.</p>
<p>Neural networks are used in a wide variety of applications, including image and speech recognition, natural language processing, predictive analytics, robotics, and more. They have been especially effective in tasks that require pattern recognition, such as identifying objects in images, translating between languages, and predicting future trends in data.</p>
</section>
<section id="neural-network-architecture" class="level3">
<h3 class="anchored" data-anchor-id="neural-network-architecture">Neural Network Architecture</h3>
<p>A neural network consists of one or more layers of neurons, each of which takes input from the previous layer and produces output for the next layer. The input layer receives raw data, while the output layer produces predictions or decisions based on that input. The hidden layers in between contain neurons that can learn to recognize patterns in the data and extract features that are useful for making predictions.</p>
<p>Each neuron in a neural network has a set of weights and biases that determine how it responds to input. These values are adjusted during training to improve the accuracy of the network’s predictions. The activation function of a neuron determines how it responds to input, such as by applying a threshold or sigmoid function.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Image</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>Image(filename<span class="op">=</span><span class="st">'Neural Networks/ai-artificial-neural-network-alex-castrounis.png'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="25">
<p><img src="softmax_neural_tensorflow_files/figure-html/cell-4-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The input layer: The three blue nodes on the left side of the diagram represent the input layer. This layer receives input data, such as pixel values from an image or numerical features from a dataset.</p>
<p>The hidden layer: The four white nodes in the middle of the diagram represent the hidden layer. This layer performs computations on the input data and generates output values that are passed to the output layer.</p>
<p>The output layer: The orange node on the right side of the diagram represents the output layer. This layer generates the final output of the neural network, which can be a binary classification (0 or 1) or a continuous value.</p>
<p>The arrows: The arrows in the diagram represent the connections between nodes in adjacent layers. Each arrow has an associated weight, which is a parameter learned during the training process. The weights determine the strength of the connections between the nodes and are used to compute the output values of each node.</p>
</section>
<section id="quick-lesson-relu-activation-function" class="level3">
<h3 class="anchored" data-anchor-id="quick-lesson-relu-activation-function">QUICK LESSON: RelU Activation Function</h3>
<p>The ReLU (Rectified Linear Unit) activation function is used in neural networks to introduce non-linearity into the model. Non-linearity allows neural networks to learn more complex relationships between inputs and outputs.</p>
<p>ReLU is a simple function that returns the input if it is positive, and 0 otherwise. This means that ReLU “activates” (returns a non-zero output) only if the input is positive, which can be thought of as a way for the neuron to “turn on” when the input is significant enough. In contrast, a linear function would simply scale the input by a constant factor, which would not introduce any non-linearity into the model.</p>
<p>In simple terms, ReLU allows the neural network to selectively activate certain neurons based on the importance of the input, which helps it learn more complex patterns in the data.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear(x):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> relu(x):</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.maximum(<span class="dv">0</span>, x)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, <span class="dv">100</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>y_linear <span class="op">=</span> linear(x)</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>y_relu <span class="op">=</span> relu(x)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y_linear, label<span class="op">=</span><span class="st">'Linear'</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y_relu, label<span class="op">=</span><span class="st">'ReLU'</span>)</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Input'</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Output'</span>)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="softmax_neural_tensorflow_files/figure-html/cell-5-output-1.png" width="608" height="422"></p>
</div>
</div>
</section>
<section id="code-implementation" class="level3">
<h3 class="anchored" data-anchor-id="code-implementation">Code Implementation</h3>
<p>TensorFlow is an open-source software library developed by Google that is widely used for building and training machine learning models, including neural networks. TensorFlow provides a range of tools and abstractions that make it easier to build and optimize complex models, as well as tools for deploying models in production.</p>
<p>Here’s an example of how to use TensorFlow to build a neural network for a softmax regression model:</p>
<p>First we start by importing the proper packages:</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.models <span class="im">import</span> Sequential</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.layers <span class="im">import</span> Dense</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.utils <span class="im">import</span> plot_model</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.losses <span class="im">import</span> SparseCategoricalCrossentropy</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_blobs</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we create a randomly generated dataset to create a model on:</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># make dataset for example</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>centers <span class="op">=</span> [[<span class="op">-</span><span class="dv">5</span>, <span class="dv">2</span>], [<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">2</span>], [<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">5</span>, <span class="op">-</span><span class="dv">2</span>]]</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>X_train, y_train <span class="op">=</span> make_blobs(n_samples<span class="op">=</span><span class="dv">2000</span>, centers<span class="op">=</span>centers, cluster_std<span class="op">=</span><span class="fl">1.0</span>,random_state<span class="op">=</span><span class="dv">50</span>)</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the example dataset</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_train[:, <span class="dv">0</span>], X_train[:, <span class="dv">1</span>], c<span class="op">=</span>y_train)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Example Dataset'</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Feature 1'</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Feature 2'</span>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="softmax_neural_tensorflow_files/figure-html/cell-7-output-1.png" width="587" height="442"></p>
</div>
</div>
<p>We will talk about three ways to implement a softmax regression machine learning model. The first using Stochastic Gradient Descent as the loss function. Next, using a potentially more efficient algoritm called the Adam Algoritm. Finally, using the Adam Algoritm but more efficiently.</p>
<section id="first-implementation-using-sgd" class="level4">
<h4 class="anchored" data-anchor-id="first-implementation-using-sgd">First Implementation (using SGD):</h4>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>sgd_model <span class="op">=</span> tf.keras.Sequential([</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>        Dense(<span class="dv">25</span>, activation <span class="op">=</span> <span class="st">'relu'</span>),</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>        Dense(<span class="dv">15</span>, activation <span class="op">=</span> <span class="st">'relu'</span>),</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        Dense(<span class="dv">4</span>, activation <span class="op">=</span> <span class="st">'softmax'</span>)    <span class="co"># &lt; softmax activation here</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>sgd_model.<span class="bu">compile</span>(</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=</span>tf.keras.losses.SparseCategoricalCrossentropy(),  <span class="co">#&lt;-- Note</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>sgd_history <span class="op">=</span> sgd_model.fit(</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>                    X_train,y_train,</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>                    epochs<span class="op">=</span><span class="dv">10</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 23s - loss: 1.9191</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>60/63 [===========================&gt;..] - ETA: 0s - loss: 1.1264 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 887us/step - loss: 1.1077</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 2/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.5153</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 753us/step - loss: 0.3769</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 3/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.2361</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 714us/step - loss: 0.1437</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 4/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.1125</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 730us/step - loss: 0.0787</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 5/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.0198</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 714us/step - loss: 0.0600</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 6/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.0278</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 728us/step - loss: 0.0529</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 7/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.0516</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 722us/step - loss: 0.0491</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 8/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.0428</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 722us/step - loss: 0.0453</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.0154</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 722us/step - loss: 0.0428</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 10/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.0681</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 714us/step - loss: 0.0404</code></pre>
</div>
</div>
<p>Here is a step-by-step explanation of the code:</p>
<ol type="1">
<li><p>First, we create a sequential model using the tf.keras.Sequential() function. This is a linear stack of layers where we can add layers using the .add() method.</p></li>
<li><p>Then we add three dense layers to the model using the .add() method. The first two layers have the relu activation function and the last layer has the softmax activation function.</p></li>
<li><p>We import SparseCategoricalCrossentropy from tensorflow.keras.losses. This is our loss function, which will be used to evaluate the model during training.</p></li>
<li><p>We compile the model using model.compile(), specifying the SparseCategoricalCrossentropy() as our loss function.</p></li>
<li><p>We fit the model to the training data using model.fit(), specifying the training data (X_train and y_train) and the number of epochs* (10).</p></li>
</ol>
<p>In summary, the code creates a sequential model with three dense layers, using the relu activation function in the first two layers and the softmax activation function in the output layer. The model is then compiled using the SparseCategoricalCrossentropy() loss function, and finally, the model is trained for 10 epochs using the model.fit() method.</p>
<p>*In machine learning, the term “epochs” refers to the number of times the entire training dataset is used to train the model. During each epoch, the model processes the entire dataset, updates its parameters based on the computed errors, and moves on to the next epoch until the desired level of accuracy is achieved. Increasing the number of epochs may improve the model accuracy, but it also increases the risk of overfitting on the training data. Therefore, the number of epochs is a hyperparameter that must be tuned to achieve the best possible results.</p>
<p>Lets check the model summary:</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>sgd_model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential_3"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>_________________________________________________________________</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> Layer (type)                Output Shape              Param #   </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>=================================================================</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> dense_9 (Dense)             (None, 25)                75        </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> dense_10 (Dense)            (None, 15)                390       </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> dense_11 (Dense)            (None, 4)                 64        </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>=================================================================</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Total params: 529</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trainable params: 529</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Non-trainable params: 0</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>_________________________________________________________________</code></pre>
</div>
</div>
<p>The None values in the output shape column represent the variable batch size that is inputted during the training process. The number of parameters in each layer depends on the number of inputs and the number of neurons in the layer, along with any additional bias terms.</p>
<p>In this example, the first hidden layer has 25 neurons, so there are 25 * 3 = 75 parameters (3 input features). The second hidden layer has 15 neurons, so there are 15 * 25 + 15 = 390 parameters (25 inputs from the previous layer, plus 15 bias terms). The output layer has 4 neurons, so there are 15 * 4 + 4 = 64 parameters (15 inputs from the previous layer, plus 4 bias terms).</p>
<p>The output None for the total number of trainable parameters means that none of the layers have been marked as non-trainable.</p>
<p>Lets predict the model:</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>p_nonpreferred <span class="op">=</span> sgd_model.predict(X_train)</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(p_nonpreferred [:<span class="dv">2</span>])</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"largest value"</span>, np.<span class="bu">max</span>(p_nonpreferred), <span class="st">"smallest value"</span>, np.<span class="bu">min</span>(p_nonpreferred))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 3s</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 678us/step</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[[2.63723987e-03 3.90884990e-04 9.96855140e-01 1.16643016e-04]
 [4.36354203e-05 9.99956131e-01 2.11513992e-07 2.64272106e-08]]
largest value 0.9999995 smallest value 5.446325e-13</code></pre>
</div>
</div>
<p>p_nonpreferred = model.predict(X_train): This line uses the predict method of the model object to make predictions on the input data X_train. The resulting predictions are stored in the p_nonpreferred variable.</p>
<p>print(p_nonpreferred [:2]): This line prints out the first two predictions from p_nonpreferred. This is a quick way to visually inspect the predictions and see what the model is outputting.</p>
<p>print(“largest value”, np.max(p_nonpreferred), “smallest value”, np.min(p_nonpreferred)): This line prints out the largest and smallest values from p_nonpreferred, which can give an idea of the range of the predictions. The np.max and np.min functions from NumPy are used to find the maximum and minimum values in p_nonpreferred.</p>
<p>The output is a matrix with two rows (because we have two input examples) and four columns (because the output layer has four neurons). Each element of the matrix is the probability that the input example belongs to the corresponding class. For example, the probability that the first input example belongs to class 3 (which has the highest probability) is 0.99254191.</p>
</section>
<section id="adam-algorithm-implementation" class="level4">
<h4 class="anchored" data-anchor-id="adam-algorithm-implementation">ADAM Algorithm Implementation</h4>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>adam_model <span class="op">=</span> Sequential(</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>    [ </span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>        Dense(<span class="dv">25</span>, activation <span class="op">=</span> <span class="st">'relu'</span>),</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>        Dense(<span class="dv">15</span>, activation <span class="op">=</span> <span class="st">'relu'</span>),</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>        Dense(<span class="dv">4</span>, activation <span class="op">=</span> <span class="st">'softmax'</span>)    <span class="co"># &lt; softmax activation here</span></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>adam_model.<span class="bu">compile</span>(</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=</span>tf.keras.losses.SparseCategoricalCrossentropy(),</span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="op">=</span>tf.keras.optimizers.Adam(<span class="fl">0.001</span>), <span class="co"># &lt; change to 0.01 and rerun</span></span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a>adam_history <span class="op">=</span> adam_model.fit(</span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a>                    X_train,y_train,</span>
<span id="cb59-15"><a href="#cb59-15" aria-hidden="true" tabindex="-1"></a>                    epochs<span class="op">=</span><span class="dv">10</span></span>
<span id="cb59-16"><a href="#cb59-16" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 33s - loss: 1.5224</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>56/63 [=========================&gt;....] - ETA: 0s - loss: 1.0510 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 1s 944us/step - loss: 1.0113</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 2/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.8011</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>59/63 [===========================&gt;..] - ETA: 0s - loss: 0.4447</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 897us/step - loss: 0.4342</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 3/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.2713</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 768us/step - loss: 0.2037</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 4/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.1290</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 778us/step - loss: 0.1219</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 5/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.0868</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 762us/step - loss: 0.0902</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 6/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.1241</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 769us/step - loss: 0.0743</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 7/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.0726</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 774us/step - loss: 0.0648</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 8/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.1275</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>59/63 [===========================&gt;..] - ETA: 0s - loss: 0.0583</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 892us/step - loss: 0.0585</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.0552</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 794us/step - loss: 0.0536</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 10/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.0413</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 765us/step - loss: 0.0497</code></pre>
</div>
</div>
<p>And the model summary:</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>adam_model.summary()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential_4"</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>_________________________________________________________________</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> Layer (type)                Output Shape              Param #   </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>=================================================================</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> dense_12 (Dense)            (None, 25)                75        </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> dense_13 (Dense)            (None, 15)                390       </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> dense_14 (Dense)            (None, 4)                 64        </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>                                                                 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>=================================================================</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Total params: 529</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Trainable params: 529</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Non-trainable params: 0</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>_________________________________________________________________</code></pre>
</div>
</div>
<p>And the predictions:</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb109"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb109-1"><a href="#cb109-1" aria-hidden="true" tabindex="-1"></a>p_nonpreferred <span class="op">=</span> adam_model.predict(X_train)</span>
<span id="cb109-2"><a href="#cb109-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(p_nonpreferred [:<span class="dv">2</span>])</span>
<span id="cb109-3"><a href="#cb109-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"largest value"</span>, np.<span class="bu">max</span>(p_nonpreferred), <span class="st">"smallest value"</span>, np.<span class="bu">min</span>(p_nonpreferred))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 3s</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 644us/step</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[[6.5815304e-03 1.3287869e-04 9.9168444e-01 1.6011768e-03]
 [1.7514135e-04 9.9980205e-01 1.6012802e-06 2.1159864e-05]]
largest value 0.9999962 smallest value 1.4536262e-08</code></pre>
</div>
</div>
<p>Here, the only difference between the these two machine learning models is the optimizer. That line of code, optimizer=tf.keras.optimizers.Adam(0.001), specifies the optimizer to be used during training. In this case, it uses the Adam optimizer with a learning rate of 0.001. The Adam optimizer is an adaptive optimization algorithm that is commonly used in deep learning for its ability to dynamically adjust the learning rate during training, which can help prevent the model from getting stuck in local minima.</p>
<section id="what-does-the-adam-algorithm-look-like" class="level5">
<h5 class="anchored" data-anchor-id="what-does-the-adam-algorithm-look-like">What does the ADAM Algorithm look like?</h5>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb113"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb113-1"><a href="#cb113-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb113-2"><a href="#cb113-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb113-3"><a href="#cb113-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-4"><a href="#cb113-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the objective function (quadratic)</span></span>
<span id="cb113-5"><a href="#cb113-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> objective(x, y):</span>
<span id="cb113-6"><a href="#cb113-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> y<span class="op">**</span><span class="dv">2</span></span>
<span id="cb113-7"><a href="#cb113-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-8"><a href="#cb113-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the Adam update rule</span></span>
<span id="cb113-9"><a href="#cb113-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> adam_update(x, y, m, v, t, alpha<span class="op">=</span><span class="fl">0.1</span>, beta1<span class="op">=</span><span class="fl">0.9</span>, beta2<span class="op">=</span><span class="fl">0.999</span>, eps<span class="op">=</span><span class="fl">1e-8</span>):</span>
<span id="cb113-10"><a href="#cb113-10" aria-hidden="true" tabindex="-1"></a>    g <span class="op">=</span> np.array([<span class="dv">2</span><span class="op">*</span>x, <span class="dv">2</span><span class="op">*</span>y])</span>
<span id="cb113-11"><a href="#cb113-11" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> beta1 <span class="op">*</span> m <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> beta1) <span class="op">*</span> g</span>
<span id="cb113-12"><a href="#cb113-12" aria-hidden="true" tabindex="-1"></a>    v <span class="op">=</span> beta2 <span class="op">*</span> v <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> beta2) <span class="op">*</span> g<span class="op">**</span><span class="dv">2</span></span>
<span id="cb113-13"><a href="#cb113-13" aria-hidden="true" tabindex="-1"></a>    m_hat <span class="op">=</span> m <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> beta1<span class="op">**</span>t)</span>
<span id="cb113-14"><a href="#cb113-14" aria-hidden="true" tabindex="-1"></a>    v_hat <span class="op">=</span> v <span class="op">/</span> (<span class="dv">1</span> <span class="op">-</span> beta2<span class="op">**</span>t)</span>
<span id="cb113-15"><a href="#cb113-15" aria-hidden="true" tabindex="-1"></a>    dx <span class="op">=</span> <span class="op">-</span> alpha <span class="op">*</span> m_hat[<span class="dv">0</span>] <span class="op">/</span> (np.sqrt(v_hat[<span class="dv">0</span>]) <span class="op">+</span> eps)</span>
<span id="cb113-16"><a href="#cb113-16" aria-hidden="true" tabindex="-1"></a>    dy <span class="op">=</span> <span class="op">-</span> alpha <span class="op">*</span> m_hat[<span class="dv">1</span>] <span class="op">/</span> (np.sqrt(v_hat[<span class="dv">1</span>]) <span class="op">+</span> eps)</span>
<span id="cb113-17"><a href="#cb113-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dx, dy, m, v</span>
<span id="cb113-18"><a href="#cb113-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-19"><a href="#cb113-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the parameters for the optimization</span></span>
<span id="cb113-20"><a href="#cb113-20" aria-hidden="true" tabindex="-1"></a>theta <span class="op">=</span> np.array([<span class="fl">2.0</span>, <span class="fl">2.0</span>])</span>
<span id="cb113-21"><a href="#cb113-21" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> np.zeros(<span class="dv">2</span>)</span>
<span id="cb113-22"><a href="#cb113-22" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> np.zeros(<span class="dv">2</span>)</span>
<span id="cb113-23"><a href="#cb113-23" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb113-24"><a href="#cb113-24" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.1</span></span>
<span id="cb113-25"><a href="#cb113-25" aria-hidden="true" tabindex="-1"></a>beta1 <span class="op">=</span> <span class="fl">0.9</span></span>
<span id="cb113-26"><a href="#cb113-26" aria-hidden="true" tabindex="-1"></a>beta2 <span class="op">=</span> <span class="fl">0.999</span></span>
<span id="cb113-27"><a href="#cb113-27" aria-hidden="true" tabindex="-1"></a>eps <span class="op">=</span> <span class="fl">1e-8</span></span>
<span id="cb113-28"><a href="#cb113-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-29"><a href="#cb113-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate the parameter space grid</span></span>
<span id="cb113-30"><a href="#cb113-30" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb113-31"><a href="#cb113-31" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb113-32"><a href="#cb113-32" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> np.meshgrid(x, y)</span>
<span id="cb113-33"><a href="#cb113-33" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> objective(X, Y)</span>
<span id="cb113-34"><a href="#cb113-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-35"><a href="#cb113-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate the parameter space plot</span></span>
<span id="cb113-36"><a href="#cb113-36" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots()</span>
<span id="cb113-37"><a href="#cb113-37" aria-hidden="true" tabindex="-1"></a>ax.contour(X, Y, Z, levels<span class="op">=</span><span class="dv">30</span>, cmap<span class="op">=</span><span class="st">'jet'</span>)</span>
<span id="cb113-38"><a href="#cb113-38" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">'x'</span>)</span>
<span id="cb113-39"><a href="#cb113-39" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">'y'</span>)</span>
<span id="cb113-40"><a href="#cb113-40" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">'Parameter Space of Adam'</span>)</span>
<span id="cb113-41"><a href="#cb113-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb113-42"><a href="#cb113-42" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform several iterations of Adam and plot the updates</span></span>
<span id="cb113-43"><a href="#cb113-43" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">20</span>):</span>
<span id="cb113-44"><a href="#cb113-44" aria-hidden="true" tabindex="-1"></a>    t <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb113-45"><a href="#cb113-45" aria-hidden="true" tabindex="-1"></a>    dx, dy, m, v <span class="op">=</span> adam_update(theta[<span class="dv">0</span>], theta[<span class="dv">1</span>], m, v, t, alpha, beta1, beta2, eps)</span>
<span id="cb113-46"><a href="#cb113-46" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">+=</span> np.array([dx, dy])</span>
<span id="cb113-47"><a href="#cb113-47" aria-hidden="true" tabindex="-1"></a>    ax.arrow(theta[<span class="dv">0</span>]<span class="op">-</span>dx, theta[<span class="dv">1</span>]<span class="op">-</span>dy, dx, dy, head_width<span class="op">=</span><span class="fl">0.1</span>, head_length<span class="op">=</span><span class="fl">0.1</span>, fc<span class="op">=</span><span class="st">'b'</span>, ec<span class="op">=</span><span class="st">'b'</span>)</span>
<span id="cb113-48"><a href="#cb113-48" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="softmax_neural_tensorflow_files/figure-html/cell-14-output-1.png" width="592" height="442"></p>
</div>
</div>
</section>
</section>
<section id="lets-compare-the-loss" class="level4">
<h4 class="anchored" data-anchor-id="lets-compare-the-loss">Lets compare the loss</h4>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb114"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1" aria-hidden="true" tabindex="-1"></a>plt.plot(sgd_history.history[<span class="st">'loss'</span>], label<span class="op">=</span><span class="st">'SGD'</span>)</span>
<span id="cb114-2"><a href="#cb114-2" aria-hidden="true" tabindex="-1"></a>plt.plot(adam_history.history[<span class="st">'loss'</span>], label<span class="op">=</span><span class="st">'Adam'</span>)</span>
<span id="cb114-3"><a href="#cb114-3" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb114-4"><a href="#cb114-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Epoch'</span>)</span>
<span id="cb114-5"><a href="#cb114-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Loss'</span>)</span>
<span id="cb114-6"><a href="#cb114-6" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="softmax_neural_tensorflow_files/figure-html/cell-15-output-1.png" width="589" height="422"></p>
</div>
</div>
</section>
</section>
<section id="improved-implementation-of-adam-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="improved-implementation-of-adam-algorithm">Improved Implementation of ADAM Algorithm</h3>
<p>As we have talked about in class before, numerical roundoff errors happen when coding in python due to memory overflow.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb115"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb115-1"><a href="#cb115-1" aria-hidden="true" tabindex="-1"></a>x1 <span class="op">=</span> <span class="fl">2.0</span> <span class="op">/</span> <span class="dv">10000</span></span>
<span id="cb115-2"><a href="#cb115-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>x1<span class="sc">:.18f}</span><span class="ss">"</span>) <span class="co"># print 18 digits to the right of the decimal point</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.000200000000000000</code></pre>
</div>
</div>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb117"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb117-1"><a href="#cb117-1" aria-hidden="true" tabindex="-1"></a>x2 <span class="op">=</span> <span class="dv">1</span> <span class="op">+</span> (<span class="dv">1</span><span class="op">/</span><span class="dv">10000</span>) <span class="op">-</span> (<span class="dv">1</span> <span class="op">-</span> <span class="dv">1</span><span class="op">/</span><span class="dv">10000</span>)</span>
<span id="cb117-2"><a href="#cb117-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>x2<span class="sc">:.18f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.000199999999999978</code></pre>
</div>
</div>
<p>It turns out that while the implementation of the loss function for softmax was correct, there is a different and better way of reducing numerical roundoff errors which leads to more accurate computations.</p>
<p>If we go back to how a loss function for softmax regression is implemented we see that the loss function is expressed in the following formula: <span class="math display">\[
\text{loss}(a_1, a_2, \dots, a_n, y) =
\begin{cases}
-\log(a_1) &amp; \text{if } y = 1 \\
-\log(a_2) &amp; \text{if } y = 2 \\
\vdots &amp; \vdots \\
-\log(a_n) &amp; \text{if } y = n
\end{cases}
\]</span></p>
<p>where <span class="math inline">\(a_j\)</span> is computed from: <span class="math display">\[
a_j = \frac{e^{z_j}}{\sum\limits_{k=1}^n e^{z_k}} = P(y=j \mid \vec{x})
\]</span></p>
<p>This can lead to numerical roundoff errors in tensorflow as the loss function is not directly computing <span class="math inline">\(a_j\)</span>.</p>
<p>In terms of code, that is exactly what <code>loss=SparseCategoricalCrossentropy()</code> is doing. Therefore, it would be more accurate if we could implement the loss function as follows: <span class="math display">\[
\text{loss}(a_1, a_2, \dots, a_n, y) =
\begin{cases}
-\log(\frac{e^{z_1}}{e^{z_1} + e^{z_2} + ... + e^{z_n}}) &amp; \text{if } y = 1 \\
-\log(\frac{e^{z_2}}{e^{z_1} + e^{z_2} + ... + e^{z_n}}) &amp; \text{if } y = 2 \\
\vdots &amp; \vdots \\
-\log(\frac{e^{z_j}}{\sum\limits_{k=1}^n e^{z_k}}) &amp; \text{if } y = n
\end{cases}
\]</span></p>
<p>We achieve this in two steps. The first is making the output layer a linear activation, and additionally adding a <code>from_logits=True</code> parameter to the <code>loss=tf.keras.losses.SparseCategoricalCrossentropy</code> line of code. By using a linear activation function instead of softmax, the model will output a vector of real numbers rather than probabilities.</p>
<p>Here is the code:</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb119"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb119-1"><a href="#cb119-1" aria-hidden="true" tabindex="-1"></a>preferred_model <span class="op">=</span> Sequential(</span>
<span id="cb119-2"><a href="#cb119-2" aria-hidden="true" tabindex="-1"></a>    [ </span>
<span id="cb119-3"><a href="#cb119-3" aria-hidden="true" tabindex="-1"></a>        Dense(<span class="dv">25</span>, activation <span class="op">=</span> <span class="st">'relu'</span>),</span>
<span id="cb119-4"><a href="#cb119-4" aria-hidden="true" tabindex="-1"></a>        Dense(<span class="dv">15</span>, activation <span class="op">=</span> <span class="st">'relu'</span>),</span>
<span id="cb119-5"><a href="#cb119-5" aria-hidden="true" tabindex="-1"></a>        Dense(<span class="dv">4</span>, activation <span class="op">=</span> <span class="st">'linear'</span>)   <span class="co">#&lt;-- Note</span></span>
<span id="cb119-6"><a href="#cb119-6" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb119-7"><a href="#cb119-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb119-8"><a href="#cb119-8" aria-hidden="true" tabindex="-1"></a>preferred_model.<span class="bu">compile</span>(</span>
<span id="cb119-9"><a href="#cb119-9" aria-hidden="true" tabindex="-1"></a>    loss<span class="op">=</span>tf.keras.losses.SparseCategoricalCrossentropy(from_logits<span class="op">=</span><span class="va">True</span>),  <span class="co">#&lt;-- Note</span></span>
<span id="cb119-10"><a href="#cb119-10" aria-hidden="true" tabindex="-1"></a>    optimizer<span class="op">=</span>tf.keras.optimizers.Adam(<span class="fl">0.001</span>),</span>
<span id="cb119-11"><a href="#cb119-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb119-12"><a href="#cb119-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb119-13"><a href="#cb119-13" aria-hidden="true" tabindex="-1"></a>preferred_model.fit(</span>
<span id="cb119-14"><a href="#cb119-14" aria-hidden="true" tabindex="-1"></a>    X_train,y_train,</span>
<span id="cb119-15"><a href="#cb119-15" aria-hidden="true" tabindex="-1"></a>    epochs<span class="op">=</span><span class="dv">10</span></span>
<span id="cb119-16"><a href="#cb119-16" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 1/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 33s - loss: 2.1650</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>55/63 [=========================&gt;....] - ETA: 0s - loss: 1.3396 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 1s 963us/step - loss: 1.2780</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 2/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.7342</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>50/63 [======================&gt;.......] - ETA: 0s - loss: 0.5535</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 1ms/step - loss: 0.5128</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 3/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.4236</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 782us/step - loss: 0.2176</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 4/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.1932</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 780us/step - loss: 0.1306</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 5/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.0597</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 774us/step - loss: 0.0980</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 6/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.0394</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 775us/step - loss: 0.0821</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 7/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.0468</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 775us/step - loss: 0.0726</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 8/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.1861</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 791us/step - loss: 0.0660</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 9/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.0970</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 777us/step - loss: 0.0612</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Epoch 10/10</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 0s - loss: 0.0718</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 794us/step - loss: 0.0576</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>&lt;keras.callbacks.History at 0x13fe5ac20&gt;</code></pre>
</div>
</div>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb153"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb153-1"><a href="#cb153-1" aria-hidden="true" tabindex="-1"></a>p_preferred <span class="op">=</span> preferred_model.predict(X_train)</span>
<span id="cb153-2"><a href="#cb153-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"two example output vectors:</span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>p_preferred[:<span class="dv">2</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb153-3"><a href="#cb153-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"largest value"</span>, np.<span class="bu">max</span>(p_preferred), <span class="st">"smallest value"</span>, np.<span class="bu">min</span>(p_preferred))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> 1/63 [..............................] - ETA: 3s</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>63/63 [==============================] - 0s 671us/step</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>two example output vectors:
 [[-1.6667954 -3.3129096  2.4380977 -3.8616145]
 [-2.9834914  6.937649  -7.2081456 -3.4101813]]
largest value 9.20027 smallest value -10.961024</code></pre>
</div>
</div>
<p>Notice that in the preferred model, the outputs are not probabilities, but can range from large negative numbers to large positive numbers. The output must be sent through a softmax when performing a prediction that expects a probability.</p>
<p>If the desired output are probabilities, the output should be be processed by a <a href="https://www.tensorflow.org/api_docs/python/tf/nn/softmax">softmax</a>.</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb157"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb157-1"><a href="#cb157-1" aria-hidden="true" tabindex="-1"></a>sm_preferred <span class="op">=</span> tf.nn.softmax(p_preferred).numpy()</span>
<span id="cb157-2"><a href="#cb157-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"two example output vectors:</span><span class="ch">\n</span><span class="ss"> </span><span class="sc">{</span>sm_preferred[:<span class="dv">2</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb157-3"><a href="#cb157-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"largest value"</span>, np.<span class="bu">max</span>(sm_preferred), <span class="st">"smallest value"</span>, np.<span class="bu">min</span>(sm_preferred))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>two example output vectors:
 [[1.6144540e-02 3.1126300e-03 9.7894466e-01 1.7981583e-03]
 [4.9121067e-05 9.9991810e-01 7.1866123e-07 3.2059670e-05]]
largest value 0.9999902 smallest value 1.2572932e-08</code></pre>
</div>
</div>
<p>This code applies the softmax activation function to the output of a neural network model p_preferred, and then converts the resulting tensor to a numpy array using the .numpy() method. The resulting array sm_preferred contains the probabilities for each of the possible output classes for the input data.</p>
<p>The second line of code then prints the first two rows of sm_preferred, which correspond to the probabilities for the first two input examples in the dataset.</p>
<p>Citations: 1. https://www.tensorflow.org/api_docs/python/tf/nn/softmax 2. https://www.tensorflow.org/ 3. https://www.whyofai.com/blog/ai-explained 4. https://www.coursera.org/specializations/machine-learning-introduction</p>
</section>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>
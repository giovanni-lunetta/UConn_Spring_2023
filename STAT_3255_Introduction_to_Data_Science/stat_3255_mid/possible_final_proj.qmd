---
title: "Midterm"
author: "Giovanni Lunetta"
date: "February 28, 2023"
format:
  html:
    code-fold: true
  pdf: 
    geometry: 
      - top=30mm
      - left=20mm
jupyter: python3
---

# Exercise 9 
(Mid-term team project) The NYC Open Data of 311 Service Requests contains all requests from 2010 to present. We consider a subset of it with request time between 00:00:00 01/15/2023 and 24:00:00 01/21/2023. The subset is available in CSV format as data/nyc311_011523-012123_by022023.csv. Read the data dictionary to understand the meaning of the variables.

# 9.1 
Clean the data: fill missing fields as much as possible; check for obvious data entry errors (e.g., can Closed Date be earlier than Created Date?); summarize your suggestions to the data curator in several bullet points.
```{python}
import pandas as pd

df = pd.read_csv("nyc311_011523-012123_by022023.csv")

# Unique Key does not provide us with an important information.
# Location is the combination of Longitude & Latitude Column.
# Agency Name is the full name of Agency
df.drop(['Unique Key', 'Location', 'Agency Name'], axis=1, inplace=True)

# percent missing for each variable
print((df.isnull().sum() * 100)/ len(df))

# All of these columns have over 99% data missing, therefore, given the time contraint, it is not reasonable to try to fill this data in.
df.drop(['Bridge Highway Segment', 'Road Ramp', 'Bridge Highway Direction', 'Bridge Highway Name', 'Taxi Pick Up Location', 'Taxi Company Borough', 'Vehicle Type', 'Due Date', 'Facility Type'], axis=1, inplace=True)

# # descriptive statistics for continuous variables
# df.describe()

# percent missing for each variable
print((df.isnull().sum() * 100)/ len(df))

# convert "Created Date" and "Closed Date" columns to datetime objects
df['Created Date'] = pd.to_datetime(df['Created Date'])
df['Closed Date'] = pd.to_datetime(df['Closed Date'], errors='coerce')

# drop rows with missing "Closed Date" values
df = df.dropna(subset=['Closed Date'])

# Check if there are any instances where the "Closed Date" is earlier than the "Created Date"
closed_earlier = df[df['Closed Date'] < df['Created Date']]

if closed_earlier.empty:
    print("No instances where 'Closed Date' is earlier than 'Created Date'")
else:
    print("There are instances where 'Closed Date' is earlier than 'Created Date'")

# # prints the number of entries that have this problem
# # prints the dataset to inspect further
# print(len(closed_earlier))
# print(closed_earlier)

# creates an index containing all of examples where closed date is before created data and drops those columns
index_to_drop = df.index[df['Closed Date'] < df['Created Date']].tolist()
df.drop(index_to_drop, inplace=True)

# # view all unique values of 'Location Type'
# with pd.option_context('display.max_rows', None):
#     print(df["Location Type"].value_counts(dropna=False))

# fill null values with 'Other'
df['Location Type'] = df['Location Type'].fillna('Other')

# change all 'Other (Explain Below)' to 'Other' because they are the same thing
df['Location Type'] = df['Location Type'].replace('Other (Explain Below)', 'Other')

# with pd.option_context('display.max_rows', None):
#     print(df["Descriptor"].value_counts(dropna=False))

# fill null values with 'Other'
df['Descriptor'] = df['Descriptor'].fillna('Other')

# change all versions of other to uniform 'Other'
df['Descriptor'] = df['Descriptor'].replace({'Other (Explain Below)': 'Other',
                                             'Other (complaint details)': 'Other',
                                             'Other/Unknown': 'Other'})

with pd.option_context('display.max_rows', None):
    print(df["Address Type"].value_counts(dropna=False))

# fill null values with 'UNRECOGNIZED'
df['Address Type'] = df['Address Type'].fillna('UNRECOGNIZED')

# with pd.option_context('display.max_rows', None):
#     print(df["City"].value_counts(dropna=False))

# convert lowercase columni
df['City'] = df['City'].str.lower()

# fill city names from logitude and latitude

# with pd.option_context('display.max_rows', None):
#     print(df["Landmark"].value_counts(dropna=False))

# with pd.option_context('display.max_rows', None):
#     print(df['Resolution Description'].value_counts(dropna=False))

df['Resolution Description'].nunique()

# with pd.option_context('display.max_rows', None):
#     print(df['Resolution Action Updated Date'].value_counts(dropna=False)) 

df['Resolution Action Updated Date'].nunique()
df['Resolution Action Updated Date'] = pd.to_datetime(df['Resolution Action Updated Date'])

with pd.option_context('display.max_rows', None):
    print(df['Park Facility Name'].value_counts(dropna=False))

df['Park Facility Name'] = df['Park Facility Name'].fillna('Unspecified')

# percent missing for each variable
print((df.isnull().sum() * 100)/ len(df))

import geopy
import geocoder
import pandas as pd

# create a function to perform geocoding
def geocode(address):
    try:
        location = geocoder.osm(address)
        return location.lat, location.lng
    except:
        return None, None

# create a function to perform reverse geocoding
def reverse_geocode(latitude, longitude):
    try:
        location = geopy.Point(latitude, longitude)
        address = geopy.geocoders.Nominatim(user_agent="my-application").reverse(location)
        return address.address
    except:
        return None

# iterate over the DataFrame and fill in the missing values
for index, row in df.iterrows():
    if pd.isnull(row['Latitude']) or pd.isnull(row['Longitude']):
        lat, lng = geocode(row['Incident Address'])
        df.at[index, 'Latitude'] = lat
        df.at[index, 'Longitude'] = lng
    if pd.isnull(row['Incident Address']):
        address = reverse_geocode(row['Latitude'], row['Longitude'])
        df.at[index, 'Incident Address'] = address

# percent missing for each variable
print((df.isnull().sum() * 100)/ len(df))
```
Here is what I suggest:

1. My first suggestion is that there is no need for the 'Agency Name' and the 'Location' columns. These columns are redundant and do not provide any additional information.

2. Secondly, I question as to how it is possible for a close date to be earlier than the created date. I would make the person who is inputting the data reenter when the case was created when they are closing it in order to avoid this issue.

3. My third and overarching suggestion is to not allow someone to enter nothing. Whether it is inputting 'Other' or 'Unspecified' keep it consistent. This can be accomplished by not allowing the person inputting the data to leave an input blank or having the system auto-input one of those two fillers if it is left blank.

4. My fourth suggestion is similar. Columns such as 'Descriptor' have many different forms of 'Other such as 'Other (Explain Below)', 'Other (complaint details)', and 'Other/Unknown'. I understand how this could happen because those who click other have another dropdown area where they fill this in. But, if this is selected, then what they fill in for other must be put into that area, not the choice 'Other (Explain Below)'.

5. Finally, I would make sure everything inputted is either in all lowercase or all uppercase in order to avoid differences in values such as 'Queens' and 'queens'.

# 9.1.1 Cleaning the Test Data
```{python}

```

# 9.2
Remove requests that are not made to NYPD and create a new variable duration, which represents the time period from the Crated Date to Closed Date. Note that duration may be censored for some requests. Visualize the distribution of uncensored duration by weekdays/weekend and by borough, and test whether the distributions are the same across weekdays/weekends and across borough.
```{python}
# check all values
with pd.option_context('display.max_rows', None):
    print(df['Agency'].value_counts(dropna=False))

# drop all that are not NYPD
df = df[df['Agency'] == 'NYPD']

# convert the 'Created Date' and 'Closed Date' columns to datetime format
df['Created Date'] = pd.to_datetime(df['Created Date'], format='%m/%d/%Y %I:%M:%S %p')
df['Closed Date'] = pd.to_datetime(df['Closed Date'], format='%m/%d/%Y %I:%M:%S %p')

# create a new column 'duration' by subtracting the 'Created Date' column from the 'Closed Date' column
df['duration'] = df['Closed Date'] - df['Created Date']

# convert the resulting timedelta object to a numerical value in seconds
df['duration_seconds'] = df['duration'].dt.total_seconds()

df['duration_days'] = df['duration'].dt.days
# print the first few rows of the resulting DataFrame
df.head()

import numpy as np

# Convert the 'duration' column to seconds
df['duration_sec'] = df['duration'].dt.total_seconds()

# Create two new columns - one for the day of the week and one for the type of day (weekday/weekend):
df['day_of_week'] = df['Created Date'].dt.dayofweek
df['day_type'] = np.where(df['day_of_week'] < 5, 'weekday', 'weekend')

import seaborn as sns
import matplotlib.pyplot as plt

# Create a histogram for each borough using sns.histplot():
for borough in df['Borough'].unique():
    sns.histplot(data=df[(df['Borough'] == borough) & (df['duration_sec'] >= 0)], 
                 x='duration_sec', 
                 hue='day_type', 
                 kde=True, 
                 bins=50, 
                 alpha=0.5)
    plt.title(f'Distribution of Uncensored Duration for {borough}')
    plt.xlabel('Duration (seconds)')
    plt.show()

# Create a new DataFrame with only uncensored requests:
uncensored_df = df[df['duration'] >= pd.Timedelta(0)]

# Create a boxplot for each borough using sns.boxplot():
sns.boxplot(data=uncensored_df, x='Borough', y='duration_sec', hue='day_type')
plt.title('Distribution of Uncensored Duration by Borough and Day Type')
plt.xlabel('Borough')
plt.ylabel('Duration (seconds)')
plt.show()

from scipy.stats import kruskal

# Create two new DataFrames - one for weekdays and one for weekends
weekday_df = uncensored_df[uncensored_df['day_type'] == 'weekday']
weekend_df = uncensored_df[uncensored_df['day_type'] == 'weekend']

# Perform the Kruskal-Wallis test on each borough
for borough in uncensored_df['Borough'].unique():
    weekday_duration = weekday_df[weekday_df['Borough'] == borough]['duration_sec']
    weekend_duration = weekend_df[weekend_df['Borough'] == borough]['duration_sec']
    _, pvalue = kruskal(weekday_duration, weekend_duration)
    print(f'p-value for {borough}: {pvalue}')

# df.to_csv('cleaned.csv')
```
Based on the p-values, we can conclude that the distribution of uncensored durations is not the same across all boroughs. Specifically, the p-values for Queens, Brooklyn, and Staten Island are less than the significance level of 0.05, suggesting that the distribution of uncensored durations differs significantly across these boroughs. However, the p-values for the Bronx, Manhattan, and Unspecified are greater than 0.05, indicating that we cannot reject the null hypothesis that the distributions are the same across these boroughs.

# 9.3
Build a model to predict the duration for 311 requests to get closed. If your model has tuning parameters, justify their choices. Apply this model to the 311 requests of NYPD in the week of 01/22/2023. Assess the performance of your model.

```{python}
import pandas as pd

df_new = pd.read_csv('cleaned.csv')
df_cleaned = df_new.copy()

df_cleaned.drop('Unnamed: 0', axis=1, inplace=True)

# Convert all string values to lowercase
df_cleaned = df_cleaned.applymap(lambda x: x.lower() if isinstance(x, str) else x)

# percent missing for each variable
# print((df_cleaned.isnull().sum() * 100)/ len(df_cleaned))

# drop columns that have to do with predictor variable along with 'BBL' and 'Agency' as they do not provide any information
df_cleaned = df_cleaned.drop(['duration', 'duration_days', 'Closed Date'], axis=1)
df_cleaned = df_cleaned.drop(['Street Name', 'BBL', 'Agency', 'Incident Address', 'X Coordinate (State Plane)', 'Y Coordinate (State Plane)', 'Status', 'Community Board', 'Cross Street 1', 'Cross Street 2', 'Intersection Street 1', 'Intersection Street 2', 'Status', 'Landmark', 'Park Borough'], axis=1)

# percent missing for each variable
# print((df_cleaned.isnull().sum() * 100)/ len(df_cleaned))

# drop rows where rows are null
df_cleaned = df_cleaned.dropna(subset=['City'])

# percent missing for each variable
print((df_cleaned.isnull().sum() * 100)/ len(df_cleaned))

from sklearn.preprocessing import StandardScaler

# Create a StandardScaler object
scaler = StandardScaler()

# Fit and transform the 'Latitude' and 'Longitude' columns using .loc
df_cleaned.loc[:, ['Latitude', 'Longitude']] = scaler.fit_transform(df_cleaned.loc[:, ['Latitude', 'Longitude']])

# df_cleaned['Latitude'].head()

# df_cleaned.dtypes

df_cleaned = df_cleaned.drop(['duration_seconds'], axis=1)

df_cleaned['Incident Zip'] = df_cleaned['Incident Zip'].astype(str)
df_cleaned['day_of_week'] = df_cleaned['day_of_week'].astype(str)
df_cleaned['day_type'] = df_cleaned['day_type'].astype(str)

# df_cleaned.dtypes

df_cleaned['Created Date'] = pd.to_datetime(df_cleaned['Created Date'])
df_cleaned['Resolution Action Updated Date'] = pd.to_datetime(df_cleaned['Resolution Action Updated Date'])

# Extract hour, day, and month from 'Created Date' column
df_cleaned['Created Hour'] = df_cleaned['Created Date'].dt.hour
df_cleaned['Created Day'] = df_cleaned['Created Date'].dt.day
df_cleaned['Created Month'] = df_cleaned['Created Date'].dt.month

# Extract hour, day, and month from 'Resolution Action Updated Date' column
df_cleaned['Resolution Updated Hour'] = df_cleaned['Resolution Action Updated Date'].dt.hour
df_cleaned['Resolution Updated Day'] = df_cleaned['Resolution Action Updated Date'].dt.day
df_cleaned['Resolution Updated Month'] = df_cleaned['Resolution Action Updated Date'].dt.month

df_cleaned.drop(['Created Date', 'Resolution Action Updated Date'], axis=1, inplace=True)

df_cleaned.drop(['Created Month', 'Resolution Updated Month'], axis=1)

# Create a StandardScaler object
scaler = StandardScaler()

# Fit and transform the 'Latitude' and 'Longitude' columns using .loc
df_cleaned.loc[:, ['Created Day', 'Created Hour', 'Resolution Updated Hour', 'Resolution Updated Day']] = scaler.fit_transform(df_cleaned.loc[:, ['Created Day', 'Created Hour', 'Resolution Updated Hour', 'Resolution Updated Day']])

# Define columns to be one-hot encoded
columns = ['Incident Zip', 'Resolution Description', 'Complaint Type', 'Descriptor', 'Location Type', 'Address Type', 'City', 'Borough', 'Open Data Channel Type', 'Park Facility Name', 'day_of_week', 'day_type']

# Apply one-hot encoding and add the prefix 'column_name'
df_cleaned = pd.get_dummies(df_cleaned, columns=columns, prefix=columns)

df_cleaned.shape

# split the data into training and testing sets using the train_test_split function from scikit-learn. We will use 80% of the data for training and 20% for testing.
from sklearn.model_selection import train_test_split

X = df_cleaned.drop(['duration_sec'], axis=1)
y = df_cleaned['duration_sec']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV

# Define the range of hyperparameters to search
param_grid = {'alpha': [0.1, 1, 10, 100]}

# Create a Ridge regression model
model = Ridge()

# Use GridSearchCV to search for the best hyperparameters
grid = GridSearchCV(model, param_grid, cv=5)
grid.fit(X_train, y_train)

# Print the best hyperparameters and score
print("Best hyperparameters: ", grid.best_params_)
print("Best score: ", grid.best_score_)

# Train the model with the best hyperparameters on the entire training set
model = Ridge(alpha=grid.best_params_['alpha'])
model.fit(X_train, y_train)

from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate MSE, RMSE, and MAE
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, y_pred)

# Print the results
print("MSE: ", mse)
print("RMSE: ", rmse)
print("MAE: ", mae)

# Calculate R-squared
from sklearn.metrics import r2_score
r2 = r2_score(y_test, y_pred)
print("R-squared: ", r2)

import matplotlib.pyplot as plt

# Scatter plot of predicted vs actual values
fig = plt.figure(figsize=(8,6))
plt.scatter(y_test, y_pred)
plt.xlabel("Actual Duration")
plt.ylabel("Predicted Duration")
plt.title("Actual vs Predicted Durations")
plt.show()

# Histogram of residuals
residuals = y_test - y_pred
plt.hist(residuals, bins=20)
plt.xlabel("Residuals")
plt.ylabel("Frequency")
plt.title("Residual Histogram")
plt.show()
```
To predict the duration for 311 requests to get closed, I built a Ridge regression model and applied it to the 311 requests of NYPD in the week of 01/22/2023. I used the train_test_split function from scikit-learn to split the data into training and testing sets, using 80% of the data for training and 20% for testing. The feature set X was obtained by dropping the target variable, 'duration_sec', from the cleaned data frame df_cleaned, while the target variable y was set to 'duration_sec'.

Ridge regression is a type of linear regression that introduces a regularization term to the cost function to prevent overfitting. The regularization term is proportional to the square of the L2 norm of the model coefficients, which shrinks them towards zero and reduces their variance. The strength of regularization is controlled by the tuning parameter alpha, which balances the bias-variance tradeoff of the model. A smaller alpha value results in less regularization and higher variance, while a larger alpha value results in more regularization and lower variance. By tuning alpha using GridSearchCV, we can find the optimal value that maximizes the performance of the model on the training set while avoiding overfitting.

In my model, I defined the range of hyperparameters to search as {'alpha': [0.1, 1, 10, 100]}, where alpha is the regularization strength. I chose these values because they cover a wide range of orders of magnitude, from very small to very large, and can help us identify the optimal range of alpha values that balance the bias-variance tradeoff. I used the Ridge model from scikit-learn as the estimator and the GridSearchCV function with 5-fold cross-validation to search for the best hyperparameters. The best hyperparameters found were {'alpha': 0.1}, with a corresponding best score of 0.9631326374395457.

I then trained the Ridge model with the best hyperparameters on the entire training set and made predictions on the test set. The performance of the model was assessed using mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), and R-squared. The model had an MSE of 2216571.5287636667, an RMSE of 1488.815478413516, an MAE of 1210.4378688857093, and an R-squared of 0.9714858762558498.

These results indicate that the model is able to explain 97.15% of the variability in the target variable using the features in the data set. The RMSE of 1488.82 seconds suggests that the model's predictions are fairly accurate, with an average error of less than 25 seconds. The optimal value of alpha was found to be 0.1, which suggests that the model is not heavily biased or overfitting to the training data. Overall, the Ridge regression model with alpha=0.1 seems to be a good choice for predicting the duration for 311 requests to get closed for NYPD.

# 9.4
Now you know the data quite well. Come up with a research question of interest that can be answered by the data, which could be analytics or visualizations. Perform the needed analyses and answer your question.

What are the most common types of 311 complaints in the city of New York, and how do they vary across different boroughs?
```{python}
df_new = df_new.applymap(lambda x: x.lower() if isinstance(x, str) else x)

df_new.dtypes
```
To answer this question, we can start by exploring the distribution of complaint types across the entire city. We can use a bar chart to visualize the top 10 most common complaint types:
```{python}
import matplotlib.pyplot as plt

complaint_counts = df_new['Complaint Type'].value_counts().nlargest(10)
plt.bar(complaint_counts.index, complaint_counts.values)
plt.xticks(rotation=90)
plt.title('Top 10 Most Common 311 Complaint Types')
plt.xlabel('Complaint Type')
plt.ylabel('Count')
plt.show()
```
The output of this code is a bar chart showing the top 10 most common 311 complaint types in the city of New York.

Next, we can explore how these complaint types vary across different boroughs. We can use a stacked bar chart to visualize the distribution of complaint types for each borough:
```{python}
import seaborn as sns
import matplotlib.pyplot as plt

complaints_by_borough = df_new.groupby(['Borough', 'Complaint Type']).size().unstack()
complaints_by_borough_pivot = complaints_by_borough.div(complaints_by_borough.sum(axis=1), axis=0)

# set the color palette to tab20
my_palette = sns.color_palette("tab20")
sns.set_palette(my_palette)

ax = complaints_by_borough_pivot.plot(kind='bar', stacked=True, figsize=(10,8))
ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=4)
plt.title('Distribution of Complaint Types by Borough')
plt.xlabel('Borough')
```
The output of this code is a stacked bar chart showing the distribution of complaint types for each borough in the city of New York.

Based on the visualizations, we can make the following observations:

1. The most common complaint type in the city of New York is "illegal parking".

2. Complaint types related to noise, such as "Noise - Residential", "Noise - Street/Sidewalk", and "Noise - Vehicle", are among the top 5 most common complaint types in all boroughs.

3. "Blocked Driveway" is the second most common complaint type in Queens, and is among the top 5 in Brooklyn and the Bronx as well, but is not among the top 5 in Manhattan.

We can perform a statistical test to determine if there is a significant difference in complaint types across different boroughs. One way to do this is by conducting a chi-squared test of independence.

The null hypothesis for the chi-squared test is that there is no association between the borough and the complaint type, and the alternative hypothesis is that there is a significant association.

To perform the test, we can create a contingency table that shows the number of complaints for each combination of borough and complaint type. We can then use the scipy.stats.chi2_contingency function to calculate the chi-squared statistic and p-value. If the p-value is less than the significance level (e.g. 0.05), we reject the null hypothesis and conclude that there is a significant association between the borough and the complaint type.
```{python}
from scipy.stats import chi2_contingency

# create a contingency table
cont_table = pd.crosstab(index=df_new['Borough'], columns=df_new['Complaint Type'])

# perform the chi-squared test
stat, p_val, dof, expected = chi2_contingency(cont_table)

print(f"Chi-squared statistic: {stat:.2f}")
print(f"P-value: {p_val:.2f}")
```
Based on the output, the p-value is less than 0.05, which means that there is evidence to reject the null hypothesis that the complaint types are the same across all boroughs. Therefore, we can conclude that the complaint types are statistically significant between the different boroughs.

# Sources
https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/

https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html